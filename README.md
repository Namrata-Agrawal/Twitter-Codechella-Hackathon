# Twitter-Codechella-Hackathon



                                   [Devpost](https://devpost.com/software/xyz-69t2vr) [YouTube](https://www.youtube.com/watch?v=LUF9ISxdzns&feature=youtu.be)
                                   [GitHub Repo](https://github.com/freyamehta99/Twitter-Codechella-Hackathon) [Project Presentation](https://drive.google.com/file/d/1TX-J0k-wd0-glyrxa2Df78Q0kY0yRzOl/view?usp=sharing)



# About Project
Twitter has various accessibility features for people like "alt text," "adjusting the color contrast," "reducing the motion of in-app animations" etc. However, Twitter can do much more to improve the accessibility of people with challenges.

Our project has the following features:
- For voice tweets, the user can display written transcripts or play the audio in another language. 
- For videos with voice, the user can display a written summary of the video, and subtitles. 
- For images, the user can display a written summary of the image. 
- For text tweet, the user can switch it into a vocal note which reads the original text tweet.
- All the format conversions are generated automatically whenever a user tweets. This differs from, for instance, the already-existing feature which allows users to write image descriptions "manually" before they tweet an image.

# Built With
1. Twitter API
2. Python
3. Flask
4. HTML/CSS
5. Tensorflow
6. Keras
7. NLP
8. Google-cloud-text-to-speech-api
9. Google-cloud-translate-api


# Installation
1. Get a Twitter Developer API keys at [Twitter Developer](https://developer.twitter.com/en/apply-for-access)
2. Clone the repository from GitHub
3. Install Packages



# Developers
1. Namrata Agrawal
2. Freya Mehta
3. Khushal Chekuri
4. Pedro Lara Benitez
5. John Pougu√© Biyong
